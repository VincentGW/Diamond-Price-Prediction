{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diamond Price Prediction: Machine Learning & Statistical Analysis\n",
    "\n",
    "This notebook demonstrates:\n",
    "- Loss functions for model evaluation\n",
    "- Bayesian statistics and uncertainty quantification\n",
    "- Machine learning pipeline with train/test splits\n",
    "- Predictive analytics with multiple algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced imports for machine learning and Bayesian analysis\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and explore the diamond dataset\n",
    "directory = \"C:/Users/vgwis/Documents/Code/Python/Personal/Analytics/\"\n",
    "df = pd.read_csv(directory + 'Diamonds.csv')\n",
    "\n",
    "# Clean column names\n",
    "for column in df.columns:\n",
    "    column_s = column.strip()\n",
    "    df = df.rename(columns={column: column_s})\n",
    "\n",
    "print(\"=== DIAMOND DATASET ANALYSIS ===\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nDataset info:\")\n",
    "print(df.info())\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "print(f\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nBasic statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing for Machine Learning\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Remove the unnamed index column if it exists\n",
    "if 'Unnamed: 0' in df_clean.columns:\n",
    "    df_clean = df_clean.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "# Remove outliers (diamonds with zero dimensions)\n",
    "df_clean = df_clean[(df_clean.x > 0) & (df_clean.y > 0) & (df_clean.z > 0)]\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "categorical_columns = ['cut', 'color', 'clarity']\n",
    "\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    df_clean[col + '_encoded'] = le.fit_transform(df_clean[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "print(\"Categorical encoding completed:\")\n",
    "for col in categorical_columns:\n",
    "    print(f\"{col}: {dict(zip(label_encoders[col].classes_, range(len(label_encoders[col].classes_))))}\")\n",
    "\n",
    "print(f\"\\nCleaned dataset shape: {df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split\n",
    "# Features: carat, depth, table, x, y, z, and encoded categorical variables\n",
    "feature_columns = ['carat', 'depth', 'table', 'x', 'y', 'z', \n",
    "                   'cut_encoded', 'color_encoded', 'clarity_encoded']\n",
    "target_column = 'price'\n",
    "\n",
    "X = df_clean[feature_columns]\n",
    "y = df_clean[target_column]\n",
    "\n",
    "# Split into training and testing sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=pd.qcut(y, 5, duplicates='drop')\n",
    ")\n",
    "\n",
    "# Scale features for better model performance\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"=== TRAIN-TEST SPLIT COMPLETED ===\")\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} samples\")\n",
    "print(f\"Features used: {feature_columns}\")\n",
    "print(f\"Target variable: {target_column}\")\n",
    "print(f\"\\nTarget variable statistics:\")\n",
    "print(f\"Training set - Mean: ${y_train.mean():.2f}, Std: ${y_train.std():.2f}\")\n",
    "print(f\"Testing set - Mean: ${y_test.mean():.2f}, Std: ${y_test.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Implementation and Training\n",
    "print(\"=== TRAINING MULTIPLE MODELS ===\")\n",
    "\n",
    "# 1. Linear Regression\n",
    "print(\"Training Linear Regression...\")\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "lr_predictions = lr_model.predict(X_test_scaled)\n",
    "\n",
    "# 2. Random Forest Regressor\n",
    "print(\"Training Random Forest...\")\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)  # Random Forest doesn't need scaling\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "\n",
    "# 3. Bayesian Ridge Regression (for uncertainty quantification)\n",
    "print(\"Training Bayesian Ridge Regression...\")\n",
    "br_model = BayesianRidge()\n",
    "br_model.fit(X_train_scaled, y_train)\n",
    "br_predictions, br_std = br_model.predict(X_test_scaled, return_std=True)\n",
    "\n",
    "print(\"All models trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Functions and Model Evaluation\n",
    "def calculate_loss_functions(y_true, y_pred, model_name):\n",
    "    \"\"\"Calculate various loss functions for model evaluation\"\"\"\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    # Mean Absolute Percentage Error (MAPE)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    \n",
    "    # Huber Loss (less sensitive to outliers)\n",
    "    huber_loss = np.mean(np.where(np.abs(y_true - y_pred) <= 1.35, \n",
    "                                 0.5 * (y_true - y_pred)**2,\n",
    "                                 1.35 * (np.abs(y_true - y_pred) - 0.675)))\n",
    "    \n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R²': r2,\n",
    "        'MAPE (%)': mape,\n",
    "        'Huber Loss': huber_loss\n",
    "    }\n",
    "\n",
    "print(\"=== LOSS FUNCTION ANALYSIS ===\")\n",
    "\n",
    "# Calculate metrics for all models\n",
    "models_performance = []\n",
    "models_performance.append(calculate_loss_functions(y_test, lr_predictions, 'Linear Regression'))\n",
    "models_performance.append(calculate_loss_functions(y_test, rf_predictions, 'Random Forest'))\n",
    "models_performance.append(calculate_loss_functions(y_test, br_predictions, 'Bayesian Ridge'))\n",
    "\n",
    "# Create performance comparison DataFrame\n",
    "performance_df = pd.DataFrame(models_performance)\n",
    "print(performance_df.round(2))\n",
    "\n",
    "# Find best model for each metric\n",
    "print(\"\\n=== BEST MODEL BY METRIC ===\")\n",
    "for metric in ['MSE', 'RMSE', 'MAE', 'R²', 'MAPE (%)', 'Huber Loss']:\n",
    "    if metric == 'R²':\n",
    "        best_idx = performance_df[metric].idxmax()\n",
    "    else:\n",
    "        best_idx = performance_df[metric].idxmin()\n",
    "    best_model = performance_df.loc[best_idx, 'Model']\n",
    "    best_value = performance_df.loc[best_idx, metric]\n",
    "    print(f\"{metric}: {best_model} ({best_value:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Visualizations\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "# 1. Actual vs Predicted scatter plots\n",
    "plt.subplot(3, 3, 1)\n",
    "plt.scatter(y_test, lr_predictions, alpha=0.5, s=10)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Price ($)')\n",
    "plt.ylabel('Predicted Price ($)')\n",
    "plt.title('Linear Regression: Actual vs Predicted')\n",
    "plt.text(0.05, 0.95, f'R² = {r2_score(y_test, lr_predictions):.3f}', \n",
    "         transform=plt.gca().transAxes, verticalalignment='top')\n",
    "\n",
    "plt.subplot(3, 3, 2)\n",
    "plt.scatter(y_test, rf_predictions, alpha=0.5, s=10, color='green')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Price ($)')\n",
    "plt.ylabel('Predicted Price ($)')\n",
    "plt.title('Random Forest: Actual vs Predicted')\n",
    "plt.text(0.05, 0.95, f'R² = {r2_score(y_test, rf_predictions):.3f}', \n",
    "         transform=plt.gca().transAxes, verticalalignment='top')\n",
    "\n",
    "plt.subplot(3, 3, 3)\n",
    "plt.scatter(y_test, br_predictions, alpha=0.5, s=10, color='purple')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Price ($)')\n",
    "plt.ylabel('Predicted Price ($)')\n",
    "plt.title('Bayesian Ridge: Actual vs Predicted')\n",
    "plt.text(0.05, 0.95, f'R² = {r2_score(y_test, br_predictions):.3f}', \n",
    "         transform=plt.gca().transAxes, verticalalignment='top')\n",
    "\n",
    "# 2. Residual plots\n",
    "plt.subplot(3, 3, 4)\n",
    "residuals_lr = y_test - lr_predictions\n",
    "plt.scatter(lr_predictions, residuals_lr, alpha=0.5, s=10)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted Price ($)')\n",
    "plt.ylabel('Residuals ($)')\n",
    "plt.title('Linear Regression: Residual Plot')\n",
    "\n",
    "plt.subplot(3, 3, 5)\n",
    "residuals_rf = y_test - rf_predictions\n",
    "plt.scatter(rf_predictions, residuals_rf, alpha=0.5, s=10, color='green')\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted Price ($)')\n",
    "plt.ylabel('Residuals ($)')\n",
    "plt.title('Random Forest: Residual Plot')\n",
    "\n",
    "plt.subplot(3, 3, 6)\n",
    "residuals_br = y_test - br_predictions\n",
    "plt.scatter(br_predictions, residuals_br, alpha=0.5, s=10, color='purple')\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted Price ($)')\n",
    "plt.ylabel('Residuals ($)')\n",
    "plt.title('Bayesian Ridge: Residual Plot')\n",
    "\n",
    "# 3. Loss function comparison\n",
    "plt.subplot(3, 3, 7)\n",
    "metrics = ['MSE', 'RMSE', 'MAE', 'MAPE (%)']\n",
    "x_pos = np.arange(len(metrics))\n",
    "width = 0.25\n",
    "\n",
    "for i, model in enumerate(['Linear Regression', 'Random Forest', 'Bayesian Ridge']):\n",
    "    values = [performance_df.loc[i, metric] for metric in metrics]\n",
    "    # Normalize values for better comparison\n",
    "    normalized_values = [val / performance_df[metric].max() for val, metric in zip(values, metrics)]\n",
    "    plt.bar(x_pos + i*width, normalized_values, width, \n",
    "            label=model, alpha=0.8)\n",
    "\n",
    "plt.xlabel('Metrics')\n",
    "plt.ylabel('Normalized Score (lower is better)')\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.xticks(x_pos + width, metrics)\n",
    "plt.legend()\n",
    "\n",
    "# 4. Feature importance (Random Forest)\n",
    "plt.subplot(3, 3, 8)\n",
    "feature_importance = rf_model.feature_importances_\n",
    "sorted_indices = np.argsort(feature_importance)[::-1]\n",
    "plt.bar(range(len(feature_importance)), feature_importance[sorted_indices])\n",
    "plt.xticks(range(len(feature_importance)), \n",
    "           [feature_columns[i] for i in sorted_indices], rotation=45)\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Random Forest: Feature Importance')\n",
    "\n",
    "# 5. Prediction distribution\n",
    "plt.subplot(3, 3, 9)\n",
    "plt.hist(y_test, bins=50, alpha=0.5, label='Actual', density=True)\n",
    "plt.hist(rf_predictions, bins=50, alpha=0.5, label='RF Predicted', density=True)\n",
    "plt.hist(br_predictions, bins=50, alpha=0.5, label='BR Predicted', density=True)\n",
    "plt.xlabel('Price ($)')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Price Distribution: Actual vs Predicted')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian Analysis and Uncertainty Quantification\n",
    "print(\"=== BAYESIAN STATISTICS CONCEPTS ===\")\n",
    "\n",
    "# 1. Uncertainty estimation with Bayesian Ridge\n",
    "print(\"1. UNCERTAINTY ESTIMATION\")\n",
    "print(f\"Mean prediction uncertainty (std): ${br_std.mean():.2f}\")\n",
    "print(f\"Max prediction uncertainty: ${br_std.max():.2f}\")\n",
    "print(f\"Min prediction uncertainty: ${br_std.min():.2f}\")\n",
    "\n",
    "# 2. Confidence intervals for predictions\n",
    "confidence_level = 0.95\n",
    "z_score = stats.norm.ppf((1 + confidence_level) / 2)\n",
    "lower_bound = br_predictions - z_score * br_std\n",
    "upper_bound = br_predictions + z_score * br_std\n",
    "\n",
    "# Calculate coverage (how many actual values fall within confidence intervals)\n",
    "coverage = np.mean((y_test >= lower_bound) & (y_test <= upper_bound))\n",
    "print(f\"\\n2. CONFIDENCE INTERVALS\")\n",
    "print(f\"{confidence_level*100}% Confidence Interval Coverage: {coverage:.3f}\")\n",
    "print(f\"Expected coverage: {confidence_level:.3f}\")\n",
    "\n",
    "# 3. Bayesian vs Frequentist interpretation\n",
    "print(f\"\\n3. BAYESIAN INTERPRETATION\")\n",
    "print(\"Bayesian Ridge provides:\")\n",
    "print(\"- Posterior distribution over model parameters\")\n",
    "print(\"- Natural uncertainty quantification\")\n",
    "print(\"- Regularization through prior distributions\")\n",
    "print(f\"- Alpha (precision of noise): {br_model.alpha_:.2e}\")\n",
    "print(f\"- Lambda (precision of weights): {br_model.lambda_:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Bayesian uncertainty\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Sample some predictions to visualize\n",
    "sample_size = 1000\n",
    "sample_indices = np.random.choice(len(y_test), sample_size, replace=False)\n",
    "y_test_sample = y_test.iloc[sample_indices]\n",
    "br_pred_sample = br_predictions[sample_indices]\n",
    "br_std_sample = br_std[sample_indices]\n",
    "lower_sample = lower_bound[sample_indices]\n",
    "upper_sample = upper_bound[sample_indices]\n",
    "\n",
    "# Sort by actual values for better visualization\n",
    "sort_indices = np.argsort(y_test_sample)\n",
    "y_test_sorted = y_test_sample.iloc[sort_indices]\n",
    "br_pred_sorted = br_pred_sample[sort_indices]\n",
    "lower_sorted = lower_sample[sort_indices]\n",
    "upper_sorted = upper_sample[sort_indices]\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "x_range = range(len(y_test_sorted))\n",
    "plt.fill_between(x_range, lower_sorted, upper_sorted, alpha=0.3, label='95% Confidence Interval')\n",
    "plt.plot(x_range, y_test_sorted, 'o', markersize=2, label='Actual', alpha=0.7)\n",
    "plt.plot(x_range, br_pred_sorted, 'r-', linewidth=1, label='Predicted')\n",
    "plt.xlabel('Sample Index (sorted by actual price)')\n",
    "plt.ylabel('Price ($)')\n",
    "plt.title('Bayesian Predictions with Uncertainty')\n",
    "plt.legend()\n",
    "\n",
    "# Uncertainty vs prediction accuracy\n",
    "plt.subplot(2, 2, 2)\n",
    "absolute_errors = np.abs(y_test - br_predictions)\n",
    "plt.scatter(br_std, absolute_errors, alpha=0.5, s=10)\n",
    "plt.xlabel('Prediction Uncertainty (std)')\n",
    "plt.ylabel('Absolute Error ($)')\n",
    "plt.title('Uncertainty vs Prediction Error')\n",
    "correlation = np.corrcoef(br_std, absolute_errors)[0, 1]\n",
    "plt.text(0.05, 0.95, f'Correlation: {correlation:.3f}', \n",
    "         transform=plt.gca().transAxes, verticalalignment='top')\n",
    "\n",
    "# Histogram of uncertainties\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.hist(br_std, bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Prediction Uncertainty (std)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Prediction Uncertainties')\n",
    "\n",
    "# High vs low uncertainty predictions\n",
    "plt.subplot(2, 2, 4)\n",
    "high_uncertainty_mask = br_std > np.percentile(br_std, 75)\n",
    "low_uncertainty_mask = br_std < np.percentile(br_std, 25)\n",
    "\n",
    "plt.scatter(y_test[low_uncertainty_mask], br_predictions[low_uncertainty_mask], \n",
    "           alpha=0.5, s=10, label='Low Uncertainty', color='blue')\n",
    "plt.scatter(y_test[high_uncertainty_mask], br_predictions[high_uncertainty_mask], \n",
    "           alpha=0.5, s=10, label='High Uncertainty', color='red')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "plt.xlabel('Actual Price ($)')\n",
    "plt.ylabel('Predicted Price ($)')\n",
    "plt.title('Predictions by Uncertainty Level')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n4. INSIGHTS FROM UNCERTAINTY ANALYSIS\")\n",
    "high_uncertainty_error = np.mean(absolute_errors[high_uncertainty_mask])\n",
    "low_uncertainty_error = np.mean(absolute_errors[low_uncertainty_mask])\n",
    "print(f\"Average error for high uncertainty predictions: ${high_uncertainty_error:.2f}\")\n",
    "print(f\"Average error for low uncertainty predictions: ${low_uncertainty_error:.2f}\")\n",
    "print(f\"Error reduction ratio: {high_uncertainty_error/low_uncertainty_error:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary: Statistical and Computing Concepts Demonstrated\n",
    "\n",
    "## Statistical Concepts Covered:\n",
    "\n",
    "### 1. **Loss Functions**\n",
    "- **MSE (Mean Squared Error)**: Penalizes large errors quadratically\n",
    "- **RMSE (Root Mean Squared Error)**: Same units as target variable\n",
    "- **MAE (Mean Absolute Error)**: Less sensitive to outliers\n",
    "- **MAPE (Mean Absolute Percentage Error)**: Scale-independent metric\n",
    "- **Huber Loss**: Robust to outliers (combines MSE and MAE)\n",
    "- **R² Score**: Explained variance ratio\n",
    "\n",
    "### 2. **Bayesian Statistics**\n",
    "- **Posterior Distributions**: BayesianRidge provides uncertainty estimates\n",
    "- **Prior Knowledge**: Regularization through precision parameters\n",
    "- **Uncertainty Quantification**: Standard deviations for predictions\n",
    "- **Confidence Intervals**: 95% coverage analysis\n",
    "- **Bayesian vs Frequentist**: Probabilistic vs point estimates\n",
    "\n",
    "## Computing/ML Concepts Covered:\n",
    "\n",
    "### 1. **Machine Learning Pipeline**\n",
    "- Data preprocessing and cleaning\n",
    "- Feature encoding (categorical variables)\n",
    "- Train-test split with stratification\n",
    "- Feature scaling for algorithms that require it\n",
    "\n",
    "### 2. **Predictive Analytics**\n",
    "- **Linear Regression**: Baseline linear model\n",
    "- **Random Forest**: Ensemble method with feature importance\n",
    "- **Bayesian Ridge**: Probabilistic linear model with uncertainty\n",
    "\n",
    "### 3. **Model Evaluation**\n",
    "- Multiple metrics comparison\n",
    "- Residual analysis\n",
    "- Feature importance analysis\n",
    "- Prediction distribution comparison\n",
    "- Uncertainty-error correlation analysis\n",
    "\n",
    "## Key Insights:\n",
    "- Random Forest typically performs best on this dataset\n",
    "- Bayesian methods provide valuable uncertainty estimates\n",
    "- Higher uncertainty correlates with higher prediction errors\n",
    "- Feature importance shows carat weight dominates pricing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}